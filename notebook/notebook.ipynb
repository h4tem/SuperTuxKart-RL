{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 19:00:04.432068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731261604.461370   15053 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731261604.469762   15053 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 19:00:04.493547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from pystk2_gymnasium import FlattenerWrapper, PolarObservations, ConstantSizedObservations, MonoAgentWrapperAdapter\n",
    "from pystk2_gymnasium.stk_wrappers import OnlyContinuousActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "OrderedDict([('acceleration', array([0.19325757], dtype=float32)), ('brake', np.int64(0)), ('drift', np.int64(0)), ('fire', np.int64(0)), ('nitro', np.int64(1)), ('rescue', np.int64(0)), ('steer', array([0.02735091], dtype=float32))])\n",
      "Dict('attachment': Discrete(10), 'attachment_time_left': Box(0.0, inf, (1,), float32), 'center_path': Box(-inf, inf, (3,), float32), 'center_path_distance': Box(-inf, inf, (1,), float32), 'distance_down_track': Box(-inf, inf, (1,), float32), 'energy': Box(0.0, inf, (1,), float32), 'front': Box(-inf, inf, (3,), float32), 'items_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'items_type': Sequence(Discrete(7), stack=False), 'jumping': Discrete(2), 'karts_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'max_steer_angle': Box(-1.0, 1.0, (1,), float32), 'paths_distance': Sequence(Box(0.0, inf, (2,), float32), stack=False), 'paths_end': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_start': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_width': Sequence(Box(0.0, inf, (1,), float32), stack=False), 'powerup': Discrete(11), 'shield_time': Box(0.0, inf, (1,), float32), 'skeed_factor': Box(0.0, inf, (1,), float32), 'velocity': Box(-inf, inf, (3,), float32)) {'powerup': 0, 'attachment': 9, 'attachment_time_left': array([0.], dtype=float32), 'max_steer_angle': array([0.41681457], dtype=float32), 'energy': array([0.], dtype=float32), 'skeed_factor': array([1.], dtype=float32), 'shield_time': array([0.], dtype=float32), 'jumping': 0, 'distance_down_track': array([0.], dtype=float32), 'velocity': array([0.00132043, 0.07964989, 0.24528162], dtype=float32), 'front': array([ 5.0095841e-06, -3.8326675e-08,  7.1849984e-01], dtype=float32), 'center_path_distance': array([2.477873], dtype=float32), 'center_path': array([ 2.475948e+00, -9.765268e-02, -9.536743e-07], dtype=float32), 'items_position': (array([ 1.5279241, -0.3484527, 71.33874  ], dtype=float32), array([-2.4720273 , -0.34819117, 71.31905   ], dtype=float32), array([-6.471979  , -0.34828025, 71.29938   ], dtype=float32), array([85.30534 , 16.62886 ,  9.217291], dtype=float32), array([88.82472  , 16.644243 ,  7.3146462], dtype=float32), array([92.3442   , 16.659704 ,  5.4020023], dtype=float32), array([-125.85125  ,   -0.3790216,   22.101511 ], dtype=float32), array([-129.8512   ,   -0.3788715,   22.081831 ], dtype=float32), array([-35.7827   ,   4.7339444, 128.71646  ], dtype=float32), array([ 48.16776 ,  15.702281, 124.25222 ], dtype=float32), array([-133.85022   ,   -0.37718612,   21.872158  ], dtype=float32), array([ 48.148075,  15.669986, 128.25204 ], dtype=float32), array([ 48.128395,  15.637703, 132.25186 ], dtype=float32), array([-30.4204  ,   7.258091, 144.54585 ], dtype=float32), array([-17.461428,   9.104015, 155.398   ], dtype=float32), array([-25.309824,  16.901525, -24.556261], dtype=float32), array([-29.309774,  16.901676, -24.575941], dtype=float32), array([-33.309727,  16.901827, -24.595621], dtype=float32), array([-13.301632  ,   0.46088332, -83.5158    ], dtype=float32), array([ -9.782177  ,   0.47720778, -85.42844   ], dtype=float32), array([ -6.262785  ,   0.49340272, -87.33108   ], dtype=float32), array([ -65.89366   ,    0.78204155, -121.56055   ], dtype=float32), array([ -83.82682   ,    0.76074034, -118.92882   ], dtype=float32), array([ -99.60093  ,    0.7050131, -112.03658  ], dtype=float32), array([-112.92686  ,    0.6133962, -100.70237  ], dtype=float32), array([-173.72736 ,   16.858936,  -19.326612], dtype=float32), array([-175.30931 ,   16.88857 ,  -22.994316], dtype=float32), array([-176.90125 ,   16.918285,  -26.672075], dtype=float32)), 'items_type': (0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 3, 3, 1, 2, 1, 0, 1, 0, 3, 3, 3, 3, 0, 1, 0), 'karts_position': (array([-2.0028071, -0.0142875,  2.1627278], dtype=float32), array([-3.9889085 , -0.01429369,  3.2623901 ], dtype=float32)), 'paths_distance': (array([0.       , 2.0260959], dtype=float32), array([2.0260959, 9.025421 ], dtype=float32), array([ 9.025421, 16.025764], dtype=float32), array([16.025764, 23.026117], dtype=float32), array([23.026117, 30.025986], dtype=float32)), 'paths_width': (array([10.000124], dtype=float32), array([10.000126], dtype=float32), array([10.000151], dtype=float32), array([10.000181], dtype=float32), array([10.000206], dtype=float32)), 'paths_start': (array([-2.4758961 ,  0.09890761,  4.046093  ], dtype=float32), array([-2.4758701 ,  0.09953602,  6.072189  ], dtype=float32), array([-2.4743114,  0.0999786, 13.071513 ], dtype=float32), array([-2.4707434 ,  0.10141303, 20.071856  ], dtype=float32), array([-2.4641848 ,  0.10184746, 27.072203  ], dtype=float32)), 'paths_end': (array([-2.4758701 ,  0.09953602,  6.072189  ], dtype=float32), array([-2.4743114,  0.0999786, 13.071513 ], dtype=float32), array([-2.4707434 ,  0.10141303, 20.071856  ], dtype=float32), array([-2.4641848 ,  0.10184746, 27.072203  ], dtype=float32), array([-2.454633 ,  0.1022858, 34.072067 ], dtype=float32))}\n"
     ]
    }
   ],
   "source": [
    "# On teste l'environnement avec des actions random\n",
    "# Normalement si t'es sur WSL le render_mode = 'human' va te causer pb vu qu'il y a pas d'interface graphique. \n",
    "# Faut faire un X11 forwarding, perso j'ai DL XLaunch pour faire ça.\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode=None, track='abyss',max_paths=5)\n",
    "ix = 0\n",
    "done = False\n",
    "state, *_ = env.reset() \n",
    "\n",
    "while not done:\n",
    "    ix += 1\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    print(env.observation_space,state)\n",
    "    done = truncated or terminated\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "# Je vais te monter le pb dont je te parlais tout à l'heure lié à pystk2_gymnasium.\n",
    "# En gros, le .reset() est buggué, exemple : \n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode=None, track='abyss',max_paths=5)\n",
    "done = False\n",
    "state, *_ = env.reset() # Tout roule\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "env.reset() # Là normalement t'as une erreur.\n",
    "env.close()\n",
    "\n",
    "# En soi normalement on s'en balek, mais le truc c'est que ce problème va faire crash le PPO de stable baseline plus bas, donc faut le fix.\n",
    "# Pour faire ça, faut aller dans envs.py, et modifier reset. Voilà la modif:\n",
    "\n",
    "# def reset(\n",
    "#         self,\n",
    "#         *,\n",
    "#         seed: Optional[int] = None,\n",
    "#         options: Optional[Dict[str, Any]] = None,\n",
    "#     ) -> Tuple[pystk2.WorldState, Dict[str, Any]]:\n",
    "        \n",
    "#         random = np.random.RandomState(seed)\n",
    "\n",
    "#         # Ajout d'un nettoyage manuel pour self.race\n",
    "#         if self.race:\n",
    "#             print(\"Nettoyage de la course en cours avant le reset.\")\n",
    "#             self.race = None  # Libère la course manuellement pour éviter les conflits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de garder uniquement les observations présentes dans keys_to_keep\n",
    "    \"\"\"\n",
    "    def __init__(self, env, keys_to_keep):\n",
    "        super().__init__(env)\n",
    "        self.keys_to_keep = keys_to_keep\n",
    "        \n",
    "        # Filtrage des dimensions et transformation en espace Box\n",
    "        low = []\n",
    "        high = []\n",
    "        for key in keys_to_keep:\n",
    "            space = self.env.observation_space[key]\n",
    "            low.extend(space.low.flatten())\n",
    "            high.extend(space.high.flatten())\n",
    "        \n",
    "        # Définir le nouvel espace d'observation comme un Box 1D\n",
    "        self.observation_space = spaces.Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Filtrage des clés et conversion en vecteur 1D\n",
    "        filtered_values = [observation[key].flatten() for key in self.keys_to_keep]\n",
    "        return np.concatenate(filtered_values)\n",
    "\n",
    "# Clés à garder dans l'observation (modifiez selon vos préférences)\n",
    "keys_to_keep = [\n",
    "    'velocity',            # Vitesse actuelle du kart\n",
    "    'center_path_distance',# Distance du chemin\n",
    "    'max_steer_angle',     # Angle maximum de direction\n",
    "    'front',\n",
    "    'center_path',\n",
    "    'paths_start', # Empiriquement, ces 5 là changent radicalement l'entraînement : On passe d'un bot qui tombe partout à un truc vraiment correct grâce\n",
    "    'paths_width', # à eux. \n",
    "    'paths_end'\n",
    "    \n",
    "]\n",
    "\n",
    "keys_to_keep2 = ['center_path', 'center_path_distance', 'distance_down_track', 'energy', 'front', \n",
    "                 'karts_position', 'max_steer_angle', 'paths_distance', 'paths_end', 'paths_start', \n",
    "                'paths_width', 'skeed_factor', 'velocity']\n",
    "\n",
    "all_obs_keys=['attachment', 'attachment_time_left', 'center_path', 'center_path_distance', 'distance_down_track', 'energy', 'front', \n",
    "              'items_position', 'items_type', 'jumping', 'karts_position', 'max_steer_angle', 'paths_distance', 'paths_end', 'paths_start',\n",
    "              'paths_width', 'powerup', 'shield_time', 'skeed_factor', 'velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToBoxActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de passer d'un dictionnaire d'actions à des actions sous forme matricielle pour PPO après\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DictToBoxActionWrapper, self).__init__(env)\n",
    "        # Définir l'espace d'action en tant que Box 1D qui combine l'accélération et la direction\n",
    "        self.action_space = Box(low=np.array([0.0, -1.0]), high=np.array([1.0, 1.0]), dtype=np.float32)\n",
    "\n",
    "    def action(self, action):\n",
    "        # Diviser l'action en 'acceleration' et 'steer' à partir du vecteur 1D\n",
    "        return {'acceleration': np.array([action[0]], dtype=np.float32), 'steer': np.array([action[1]], dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToBoxActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper qui convertit un dictionnaire d'actions en un vecteur d'actions pour PPO.\n",
    "    Il combine toutes les actions discrètes et continues en une seule Box 1D.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DictToBoxActionWrapper, self).__init__(env)\n",
    "        # Définir l'espace d'action combiné : [acceleration, steer, brake, drift, fire, nitro, rescue]\n",
    "        # acceleration et steer sont continues, les autres sont discrètes (0 ou 1) mais on les représente en float32\n",
    "        self.action_space = Box(\n",
    "            low=np.array([0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0]), \n",
    "            high=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\"\n",
    "        Convertit l'action 1D en dictionnaire d'actions pour l'environnement original.\n",
    "        \"\"\"\n",
    "        # Extraire chaque composant d'action à partir du vecteur d'entrée\n",
    "        action = {\n",
    "            'acceleration': np.array([action[0]], dtype=np.float32),\n",
    "            'steer': np.array([action[1]], dtype=np.float32),\n",
    "            'brake': 0,    # 0 ou 1\n",
    "            'drift': int(action[3] > 0.5),    # 0 ou 1\n",
    "            'fire': int(action[4] > 0.5),     # 0 ou 1\n",
    "            'nitro': int(action[5] > 0.5),    # 0 ou 1\n",
    "            'rescue': 0   # 0 ou 1\n",
    "        }\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention : CUDA n'est pas disponible. Le modèle sera entraîné sur CPU.\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | -132     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Configuration et lancement de PPO\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;66;03m# Utilise un réseau de neurones entièrement connecté (MLP) comme politique\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     env,            \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#model.save(\"../models/ppo_supertuxkart_2\")\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:200\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_locals\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:134\u001b[0m, in \u001b[0;36mBaseCallback.update_locals\u001b[0;34m(self, locals_)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_locals\u001b[39m(\u001b[38;5;28mself\u001b[39m, locals_: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Update the references to the local variables.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    :param locals_: the local variables during rollout collection\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocals_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_child_locals(locals_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Attention : CUDA n'est pas disponible. Le modèle sera entraîné sur CPU.\")\n",
    "else:\n",
    "    print(\"CUDA est disponible. L'entraînement se fera sur GPU.\")\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=None,track='abyss')\n",
    "# Entraînement uniquement sur la map 'abyss' pour le moment\n",
    "env = PolarObservations(ConstantSizedObservations(env))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "# En gros, ConstantSizedObservations permet de passer à un environnement \"réduit\", voir la doc en vrai c'est bien expliqué\n",
    "# PolarObservations ajoute des observations utiles (passage en coordonnées polaire pour avoir max_steer_angle notamment)\n",
    "\n",
    "check_env(env,warn=True) # Permet de check si tout est en ordre\n",
    "\n",
    "# Configuration et lancement de PPO\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # Utilise un réseau de neurones entièrement connecté (MLP) comme politique\n",
    "    env,            \n",
    "    verbose=1,      # Active les messages de log pour suivre l'entraînement\n",
    "    n_steps=2048,   # Nombre d'étapes dans chaque mise à jour de PPO\n",
    "    batch_size=64,  # Taille des batchs\n",
    "    gae_lambda=0.95, # Paramètre pour l'estimation de l'avantage généralisé\n",
    "    gamma=0.99,     \n",
    "    ent_coef=0.01,  \n",
    "    learning_rate=2.5e-4, \n",
    "    device=\"cuda\" \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=40000)\n",
    "\n",
    "#model.save(\"../models/ppo_supertuxkart_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'brake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m     state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     15\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/core.py:591\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    210\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    211\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:443\u001b[0m, in \u001b[0;36mSTKRaceEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrace_step()\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrace_step(\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld_update()\n\u001b[1;32m    447\u001b[0m obs, reward, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkart_ix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39muse_ai)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:109\u001b[0m, in \u001b[0;36mget_action\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(action: STKAction):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pystk2\u001b[38;5;241m.\u001b[39mAction(\n\u001b[0;32m--> 109\u001b[0m         brake\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrake\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    110\u001b[0m         nitro\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnitro\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    111\u001b[0m         drift\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrift\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    112\u001b[0m         rescue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrescue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    113\u001b[0m         fire\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfire\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    114\u001b[0m         steer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteer\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    115\u001b[0m         acceleration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'brake'"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"../models/ppo_supertuxkart_full_act\")\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=\"human\",track='abyss')\n",
    "env = PolarObservations(ConstantSizedObservations(env))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "state,_ = env.reset()\n",
    "done = False    \n",
    "while not done:\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout d'un Wrapper pour le Turbo en début de partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurboStartWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.in_countdown = True\n",
    "        self.current_step = 0\n",
    "\n",
    "    def action(self, action):\n",
    "        # Pendant la phase de décompte, applique une accélération continue pour obtenir le boost\n",
    "        if self.in_countdown:\n",
    "            action[\"acceleration\"] = np.array([1.0], dtype='float32')\n",
    "            action[\"steer\"] = np.array([0], dtype='float32')\n",
    "            self.current_step += 1\n",
    "\n",
    "            # Conditions de fin du compte à rebours (exemple : après 3 étapes, modifiez selon vos observations)\n",
    "            if self.current_step >= 200:\n",
    "                self.in_countdown = False\n",
    "\n",
    "        # Exécute l'étape avec l'action, et passe au mode normal après le décompte\n",
    "\n",
    "        return action\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uh\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "aah\n",
      "Box([ 0. -1.  0.  0.  0.  0.  0.], 1.0, (7,), float32) [ 0.79164255 -0.5695906   0.6757963   0.732773    0.5854332   0.654038\n",
      "  0.67417455]\n",
      "WTFFFFF\n",
      "[ 0.79164255 -0.5695906   0.6757963   0.732773    0.5854332   0.654038\n",
      "  0.67417455]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space,action)\n\u001b[0;32m---> 19\u001b[0m state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ix\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m150\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/gymnasium/core.py:591\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m, in \u001b[0;36mDictToBoxActionWrapper.action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWTFFFFF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 23\u001b[0m \u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macceleration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Visualisation pour voir si ça marche bien\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode='human', track='abyss',max_paths=5)\n",
    "# env = TurboStartWrapper(OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env))))\n",
    "env = DictToBoxActionWrapper(env)\n",
    "done = False\n",
    "print(\"uh\")\n",
    "state, *_ = env.reset() \n",
    "print(\"aah\")\n",
    "ix = 0\n",
    "\n",
    "while not done:\n",
    "    ix+=1\n",
    "    # action = OrderedDict([('acceleration', np.array([1.], dtype=np.float32)),('steer', np.array([0.], dtype=np.float32))])\n",
    "    action = env.action_space.sample()\n",
    "    print(env.action_space,action)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = truncated or terminated\n",
    "    if ix%150==0:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key hey\n"
     ]
    }
   ],
   "source": [
    "print(*{\"key\":0,\"hey\":1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

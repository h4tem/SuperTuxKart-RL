{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:46:20.846377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 18:46:20.868667: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 18:46:20.878386: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 18:46:20.902316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 18:46:22.286880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from pystk2_gymnasium import FlattenerWrapper, PolarObservations, ConstantSizedObservations, MonoAgentWrapperAdapter\n",
    "from pystk2_gymnasium.stk_wrappers import OnlyContinuousActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "Dict('acceleration': Box(0.0, 1.0, (1,), float32), 'brake': Discrete(2), 'drift': Discrete(2), 'fire': Discrete(2), 'nitro': Discrete(2), 'rescue': Discrete(2), 'steer': Box(-1.0, 1.0, (1,), float32))\n",
      "Dict('attachment': Discrete(10), 'attachment_time_left': Box(0.0, inf, (1,), float32), 'center_path': Box(-inf, inf, (3,), float32), 'center_path_distance': Box(-inf, inf, (1,), float32), 'distance_down_track': Box(-inf, inf, (1,), float32), 'energy': Box(0.0, inf, (1,), float32), 'front': Box(-inf, inf, (3,), float32), 'items_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'items_type': Sequence(Discrete(7), stack=False), 'jumping': Discrete(2), 'karts_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'max_steer_angle': Box(-1.0, 1.0, (1,), float32), 'paths_distance': Sequence(Box(0.0, inf, (2,), float32), stack=False), 'paths_end': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_start': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_width': Sequence(Box(0.0, inf, (1,), float32), stack=False), 'powerup': Discrete(11), 'shield_time': Box(0.0, inf, (1,), float32), 'skeed_factor': Box(0.0, inf, (1,), float32), 'velocity': Box(-inf, inf, (3,), float32)) {'powerup': 0, 'attachment': 9, 'attachment_time_left': array([0.], dtype=float32), 'max_steer_angle': array([0.40656048], dtype=float32), 'energy': array([0.], dtype=float32), 'skeed_factor': array([1.], dtype=float32), 'shield_time': array([0.], dtype=float32), 'jumping': 0, 'distance_down_track': array([0.], dtype=float32), 'velocity': array([0.09377028, 0.07412849, 0.57848495], dtype=float32), 'front': array([-2.8053455e-06,  2.9540269e-08,  7.1850026e-01], dtype=float32), 'center_path_distance': array([0.48997208], dtype=float32), 'center_path': array([ 0.48000598, -0.09830044,  0.00198722], dtype=float32), 'items_position': (array([-0.7631426 , -0.34765908, 70.20274   ], dtype=float32), array([ 3.236695  , -0.34801802, 70.23878   ], dtype=float32), array([-4.76298  , -0.3476506, 70.16671  ], dtype=float32), array([87.2678  , 16.627048,  8.460318], dtype=float32), array([90.79494  , 16.642338 ,  6.5720773], dtype=float32), array([94.322205, 16.657707,  4.673838], dtype=float32), array([-123.9401    ,   -0.37564182,   20.48118   ], dtype=float32), array([-127.93994   ,   -0.37539423,   20.445147  ], dtype=float32), array([-34.308075,   4.735477, 127.463455], dtype=float32), array([-131.93806 ,   -0.373612,   20.219126], dtype=float32), array([ 49.660194,  15.701752, 123.342445], dtype=float32), array([ 49.62416 ,  15.669471, 127.342155], dtype=float32), array([ 49.588127,  15.637201, 131.34186 ], dtype=float32), array([-29.01048 ,   7.259546, 143.31464 ], dtype=float32), array([-16.095942,   9.105187, 154.21967 ], dtype=float32), array([-23.208344,  16.902302, -25.7652  ], dtype=float32), array([-27.208181,  16.902548, -25.801233], dtype=float32), array([-31.20802 ,  16.902796, -25.837267], dtype=float32), array([-10.9595995,   0.4611726, -84.6751   ], dtype=float32), array([ -7.432353  ,   0.47740486, -86.573326  ], dtype=float32), array([ -3.9052122 ,   0.49350777, -88.461555  ], dtype=float32), array([ -63.39563   ,    0.78348875, -122.93454   ], dtype=float32), array([ -81.3394    ,    0.76263344, -120.37615   ], dtype=float32), array([ -97.14156  ,    0.7073135, -113.54847  ], dtype=float32), array([-110.51372  ,    0.6160587, -102.26884  ], dtype=float32), array([-171.64601 ,   16.863348,  -21.142391], dtype=float32), array([-173.21297 ,   16.89301 ,  -24.816534], dtype=float32), array([-174.78984,   16.92275,  -28.50077], dtype=float32)), 'items_type': (0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 3, 3, 1, 2, 1, 0, 1, 0, 3, 3, 3, 3, 0, 1, 0), 'karts_position': (array([-2.0017555, -0.0139474,  2.1404436], dtype=float32), array([ 1.9685036 , -0.01403554, -0.04349072], dtype=float32)), 'paths_distance': (array([0.       , 2.0260959], dtype=float32), array([2.0260959, 9.025421 ], dtype=float32), array([ 9.025421, 16.025764], dtype=float32), array([16.025764, 23.026117], dtype=float32), array([23.026117, 30.025986], dtype=float32)), 'paths_width': (array([10.000124], dtype=float32), array([10.000126], dtype=float32), array([10.000151], dtype=float32), array([10.000181], dtype=float32), array([10.000206], dtype=float32)), 'paths_start': (array([-0.49195698,  0.09921955,  2.9303255 ], dtype=float32), array([-0.5002145 ,  0.09985461,  4.9564047 ], dtype=float32), array([-0.5272723 ,  0.10032006, 11.955677  ], dtype=float32), array([-0.552325  ,  0.10177733, 18.955976  ], dtype=float32), array([-0.5743871,  0.1022345, 25.956293 ], dtype=float32)), 'paths_end': (array([-0.5002145 ,  0.09985461,  4.9564047 ], dtype=float32), array([-0.5272723 ,  0.10032006, 11.955677  ], dtype=float32), array([-0.552325  ,  0.10177733, 18.955976  ], dtype=float32), array([-0.5743871,  0.1022345, 25.956293 ], dtype=float32), array([-0.59345406,  0.10269556, 32.956135  ], dtype=float32))}\n"
     ]
    }
   ],
   "source": [
    "# On teste l'environnement avec des actions random\n",
    "# Normalement si t'es sur WSL le render_mode = 'human' va te causer pb vu qu'il y a pas d'interface graphique. \n",
    "# Faut faire un X11 forwarding, perso j'ai DL XLaunch pour faire ça.\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode='human', track='abyss',max_paths=5)\n",
    "ix = 0\n",
    "done = False\n",
    "state, *_ = env.reset() \n",
    "\n",
    "while not done:\n",
    "    ix += 1\n",
    "    action = env.action_space.sample()\n",
    "    print(env.action_space)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    print(env.observation_space,state)\n",
    "    done = truncated or terminated\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "# Je vais te monter le pb dont je te parlais tout à l'heure lié à pystk2_gymnasium.\n",
    "# En gros, le .reset() est buggué, exemple : \n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode=None, track='abyss',max_paths=5)\n",
    "done = False\n",
    "state, *_ = env.reset() # Tout roule\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "env.reset() # Là normalement t'as une erreur.\n",
    "env.close()\n",
    "\n",
    "# En soi normalement on s'en balek, mais le truc c'est que ce problème va faire crash le PPO de stable baseline plus bas, donc faut le fix.\n",
    "# Pour faire ça, faut aller dans envs.py, et modifier reset. Voilà la modif:\n",
    "\n",
    "# def reset(\n",
    "#         self,\n",
    "#         *,\n",
    "#         seed: Optional[int] = None,\n",
    "#         options: Optional[Dict[str, Any]] = None,\n",
    "#     ) -> Tuple[pystk2.WorldState, Dict[str, Any]]:\n",
    "        \n",
    "#         random = np.random.RandomState(seed)\n",
    "\n",
    "#         # Ajout d'un nettoyage manuel pour self.race\n",
    "#         if self.race:\n",
    "#             print(\"Nettoyage de la course en cours avant le reset.\")\n",
    "#             self.race = None  # Libère la course manuellement pour éviter les conflits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de garder uniquement les observations présentes dans keys_to_keep\n",
    "    \"\"\"\n",
    "    def __init__(self, env, keys_to_keep):\n",
    "        super().__init__(env)\n",
    "        self.keys_to_keep = keys_to_keep\n",
    "        \n",
    "        # Filtrage des dimensions et transformation en espace Box\n",
    "        low = []\n",
    "        high = []\n",
    "        for key in keys_to_keep:\n",
    "            space = self.env.observation_space[key]\n",
    "            low.extend(space.low.flatten())\n",
    "            high.extend(space.high.flatten())\n",
    "        \n",
    "        # Définir le nouvel espace d'observation comme un Box 1D\n",
    "        self.observation_space = spaces.Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Filtrage des clés et conversion en vecteur 1D\n",
    "        filtered_values = [observation[key].flatten() for key in self.keys_to_keep]\n",
    "        return np.concatenate(filtered_values)\n",
    "\n",
    "# Clés à garder dans l'observation (modifiez selon vos préférences)\n",
    "keys_to_keep = [\n",
    "    'velocity',            # Vitesse actuelle du kart\n",
    "    'center_path_distance',# Distance du chemin\n",
    "    'max_steer_angle',     # Angle maximum de direction\n",
    "    'front',\n",
    "    'center_path',\n",
    "    'paths_start', # Empiriquement, ces 5 là changent radicalement l'entraînement : On passe d'un bot qui tombe partout à un truc vraiment correct grâce\n",
    "    'paths_width', # à eux. \n",
    "    'paths_end'\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToBoxActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de passer d'un dictionnaire d'actions à des actions sous forme matricielle pour PPO après\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DictToBoxActionWrapper, self).__init__(env)\n",
    "        # Définir l'espace d'action en tant que Box 1D qui combine l'accélération et la direction\n",
    "        self.action_space = Box(low=np.array([0.0, -1.0]), high=np.array([1.0, 1.0]), dtype=np.float32)\n",
    "\n",
    "    def action(self, action):\n",
    "        # Diviser l'action en 'acceleration' et 'steer' à partir du vecteur 1D\n",
    "        return {'acceleration': np.array([action[0]], dtype=np.float32), 'steer': np.array([action[1]], dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatem/deepdac/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:453: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | -132     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.5e+03    |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 52         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00636194 |\n",
      "|    clip_fraction        | 0.0413     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.84      |\n",
      "|    explained_variance   | 0.22       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -138         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076299286 |\n",
      "|    clip_fraction        | 0.0764       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.515        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0311      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0868       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007974638 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00872    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.0711      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -139         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054467795 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00992     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 0.0462       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -139         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 231          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042669503 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.047       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.0292       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -139         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 270          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061517227 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.596        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0251      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00552     |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 0.0649       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -139         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064432137 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0209      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 0.019        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -138         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 346          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065086065 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.000656    |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00919     |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 0.0704       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -138         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 384          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066503594 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0108      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    std                  | 0.938        |\n",
      "|    value_loss           | 0.0556       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.5e+03    |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 423        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00282548 |\n",
      "|    clip_fraction        | 0.0262     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | 0.913      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00402   |\n",
      "|    std                  | 0.936      |\n",
      "|    value_loss           | 0.0134     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008136557 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 0.475       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -134         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 502          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081854835 |\n",
      "|    clip_fraction        | 0.0892       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00103     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0121      |\n",
      "|    std                  | 0.918        |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -133         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 541          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052022994 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0226      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 0.203        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -129         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 579          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054411595 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.192        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00802     |\n",
      "|    std                  | 0.914        |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007306328 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0896      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00872    |\n",
      "|    std                  | 0.911       |\n",
      "|    value_loss           | 0.404       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003821001 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00827    |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010702131 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.141       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.895       |\n",
      "|    value_loss           | 0.385       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008375855 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00889    |\n",
      "|    std                  | 0.889       |\n",
      "|    value_loss           | 0.0705      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006676246 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 0.948       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Attention : CUDA n'est pas disponible. Le modèle sera entraîné sur CPU.\")\n",
    "else:\n",
    "    print(\"CUDA est disponible. L'entraînement se fera sur GPU.\")\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=None,track='abyss')\n",
    "# Entraînement uniquement sur la map 'abyss' pour le moment\n",
    "env = OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env)))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "# En gros, ConstantSizedObservations permet de passer à un environnement \"réduit\", voir la doc en vrai c'est bien expliqué\n",
    "# PolarObservations ajoute des observations utiles (passage en coordonnées polaire pour avoir max_steer_angle notamment)\n",
    "\n",
    "check_env(env,warn=True) # Permet de check si tout est en ordre\n",
    "\n",
    "# Configuration et lancement de PPO\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # Utilise un réseau de neurones entièrement connecté (MLP) comme politique\n",
    "    env,            \n",
    "    verbose=1,      # Active les messages de log pour suivre l'entraînement\n",
    "    n_steps=2048,   # Nombre d'étapes dans chaque mise à jour de PPO\n",
    "    batch_size=64,  # Taille des batchs\n",
    "    gae_lambda=0.95, # Paramètre pour l'estimation de l'avantage généralisé\n",
    "    gamma=0.99,     \n",
    "    ent_coef=0.01,  \n",
    "    learning_rate=2.5e-4, \n",
    "    device=\"cuda\" \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=40000)\n",
    "\n",
    "#model.save(\"../models/ppo_supertuxkart_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/ppo_supertuxkart_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupertuxkart/full-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAgentSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_ai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHatem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env)))\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m FilteredObservationWrapper(env, keys_to_keep)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/envs/registration.py:802\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 802\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_spec_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    807\u001b[0m     ):\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:393\u001b[0m, in \u001b[0;36mSTKRaceEnv.__init__\u001b[0;34m(self, agent, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, agent: Optional[AgentSpec] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new race\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    :param spec: Agent spec\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    :param kwargs: General parameters, see BaseSTKRaceEnv\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# Setup the variables\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m agent \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m AgentSpec()\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:163\u001b[0m, in \u001b[0;36mBaseSTKRaceEnv.__init__\u001b[0;34m(self, render_mode, track, num_kart, max_paths, laps, difficulty)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m render_mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_modes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m=\u001b[39m render_mode\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Setup the variables\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_track \u001b[38;5;241m=\u001b[39m track\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:133\u001b[0m, in \u001b[0;36mBaseSTKRaceEnv.initialize\u001b[0;34m(self, with_graphics)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;241m=\u001b[39m PySTKProcess(with_graphics)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m BaseSTKRaceEnv\u001b[38;5;241m.\u001b[39mTRACKS:\n\u001b[0;32m--> 133\u001b[0m     BaseSTKRaceEnv\u001b[38;5;241m.\u001b[39mTRACKS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/pystk_process.py:118\u001b[0m, in \u001b[0;36mPySTKProcess._run\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe\u001b[38;5;241m.\u001b[39msend(method)\n\u001b[0;32m--> 118\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"../models/ppo_supertuxkart_2\")\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=\"human\")\n",
    "env = OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env)))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "state,_ = env.reset()\n",
    "done = False    \n",
    "while not done:\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout d'un Wrapper pour le Turbo en début de partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurboStartWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.boost_applied = False\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Démarre la course temporaire pour appliquer le boost pendant le décompte\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        print(\"Début de la phase de turbo boost pendant le décompte.\")\n",
    "        \n",
    "        while not self.boost_applied:\n",
    "            world_state = self.env._process.get_world()\n",
    "            if world_state.phase == pystk2.WorldState.Phase.COUNTDOWN_PHASE:\n",
    "                # Applique l'accélération pendant le décompte pour obtenir le boost\n",
    "                turbo_action = {\"acceleration\": 1.0, \"steer\": 0.0, \"brake\": 0, \"drift\": 0, \"fire\": 0, \"nitro\": 0, \"rescue\": 0}\n",
    "                self.env._process.race_step(get_action(turbo_action))\n",
    "            elif world_state.phase == pystk2.WorldState.Phase.GO_PHASE:\n",
    "                # Marque que le boost a été appliqué et sort de la boucle\n",
    "                self.boost_applied = True\n",
    "                print(\"Boost appliqué avec succès avant le feu vert.\")\n",
    "\n",
    "        # Réinitialise complètement l'environnement pour la vraie course\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.boost_applied = False  # Reset le flag pour les futures courses\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "uh\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Visualisation pour voir si ça marche bien\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode='human', track='abyss',max_paths=5)\n",
    "done = False\n",
    "print(\"ah\")\n",
    "state, *_ = env.reset() \n",
    "env.reset()\n",
    "print(\"uh\")\n",
    "ix = 0\n",
    "\n",
    "while not done:\n",
    "    ix+=1\n",
    "    print(ix)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = truncated or terminated\n",
    "    if ix%50==0:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "deepdac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

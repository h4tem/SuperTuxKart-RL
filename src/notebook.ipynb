{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 18:05:02.999310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-07 18:05:03.017295: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-07 18:05:03.022033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 18:05:03.044617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 18:05:03.801241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from pystk2_gymnasium import FlattenerWrapper, PolarObservations, ConstantSizedObservations, MonoAgentWrapperAdapter\n",
    "from pystk2_gymnasium.stk_wrappers import OnlyContinuousActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "Dict('acceleration': Box(0.0, 1.0, (1,), float32), 'brake': Discrete(2), 'drift': Discrete(2), 'fire': Discrete(2), 'nitro': Discrete(2), 'rescue': Discrete(2), 'steer': Box(-1.0, 1.0, (1,), float32))\n",
      "Dict('attachment': Discrete(10), 'attachment_time_left': Box(0.0, inf, (1,), float32), 'center_path': Box(-inf, inf, (3,), float32), 'center_path_distance': Box(-inf, inf, (1,), float32), 'distance_down_track': Box(-inf, inf, (1,), float32), 'energy': Box(0.0, inf, (1,), float32), 'front': Box(-inf, inf, (3,), float32), 'items_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'items_type': Sequence(Discrete(7), stack=False), 'jumping': Discrete(2), 'karts_position': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'max_steer_angle': Box(-1.0, 1.0, (1,), float32), 'paths_distance': Sequence(Box(0.0, inf, (2,), float32), stack=False), 'paths_end': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_start': Sequence(Box(-inf, inf, (3,), float32), stack=False), 'paths_width': Sequence(Box(0.0, inf, (1,), float32), stack=False), 'powerup': Discrete(11), 'shield_time': Box(0.0, inf, (1,), float32), 'skeed_factor': Box(0.0, inf, (1,), float32), 'velocity': Box(-inf, inf, (3,), float32)) {'powerup': 0, 'attachment': 9, 'attachment_time_left': array([0.], dtype=float32), 'max_steer_angle': array([0.40656048], dtype=float32), 'energy': array([0.], dtype=float32), 'skeed_factor': array([1.], dtype=float32), 'shield_time': array([0.], dtype=float32), 'jumping': 0, 'distance_down_track': array([0.], dtype=float32), 'velocity': array([0.09377028, 0.07412849, 0.57848495], dtype=float32), 'front': array([-2.8053455e-06,  2.9540269e-08,  7.1850026e-01], dtype=float32), 'center_path_distance': array([0.48997208], dtype=float32), 'center_path': array([ 0.48000598, -0.09830044,  0.00198722], dtype=float32), 'items_position': (array([-0.7631426 , -0.34765908, 70.20274   ], dtype=float32), array([ 3.236695  , -0.34801802, 70.23878   ], dtype=float32), array([-4.76298  , -0.3476506, 70.16671  ], dtype=float32), array([87.2678  , 16.627048,  8.460318], dtype=float32), array([90.79494  , 16.642338 ,  6.5720773], dtype=float32), array([94.322205, 16.657707,  4.673838], dtype=float32), array([-123.9401    ,   -0.37564182,   20.48118   ], dtype=float32), array([-127.93994   ,   -0.37539423,   20.445147  ], dtype=float32), array([-34.308075,   4.735477, 127.463455], dtype=float32), array([-131.93806 ,   -0.373612,   20.219126], dtype=float32), array([ 49.660194,  15.701752, 123.342445], dtype=float32), array([ 49.62416 ,  15.669471, 127.342155], dtype=float32), array([ 49.588127,  15.637201, 131.34186 ], dtype=float32), array([-29.01048 ,   7.259546, 143.31464 ], dtype=float32), array([-16.095942,   9.105187, 154.21967 ], dtype=float32), array([-23.208344,  16.902302, -25.7652  ], dtype=float32), array([-27.208181,  16.902548, -25.801233], dtype=float32), array([-31.20802 ,  16.902796, -25.837267], dtype=float32), array([-10.9595995,   0.4611726, -84.6751   ], dtype=float32), array([ -7.432353  ,   0.47740486, -86.573326  ], dtype=float32), array([ -3.9052122 ,   0.49350777, -88.461555  ], dtype=float32), array([ -63.39563   ,    0.78348875, -122.93454   ], dtype=float32), array([ -81.3394    ,    0.76263344, -120.37615   ], dtype=float32), array([ -97.14156  ,    0.7073135, -113.54847  ], dtype=float32), array([-110.51372  ,    0.6160587, -102.26884  ], dtype=float32), array([-171.64601 ,   16.863348,  -21.142391], dtype=float32), array([-173.21297 ,   16.89301 ,  -24.816534], dtype=float32), array([-174.78984,   16.92275,  -28.50077], dtype=float32)), 'items_type': (0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 3, 3, 1, 2, 1, 0, 1, 0, 3, 3, 3, 3, 0, 1, 0), 'karts_position': (array([-2.0017555, -0.0139474,  2.1404436], dtype=float32), array([ 1.9685036 , -0.01403554, -0.04349072], dtype=float32)), 'paths_distance': (array([0.       , 2.0260959], dtype=float32), array([2.0260959, 9.025421 ], dtype=float32), array([ 9.025421, 16.025764], dtype=float32), array([16.025764, 23.026117], dtype=float32), array([23.026117, 30.025986], dtype=float32)), 'paths_width': (array([10.000124], dtype=float32), array([10.000126], dtype=float32), array([10.000151], dtype=float32), array([10.000181], dtype=float32), array([10.000206], dtype=float32)), 'paths_start': (array([-0.49195698,  0.09921955,  2.9303255 ], dtype=float32), array([-0.5002145 ,  0.09985461,  4.9564047 ], dtype=float32), array([-0.5272723 ,  0.10032006, 11.955677  ], dtype=float32), array([-0.552325  ,  0.10177733, 18.955976  ], dtype=float32), array([-0.5743871,  0.1022345, 25.956293 ], dtype=float32)), 'paths_end': (array([-0.5002145 ,  0.09985461,  4.9564047 ], dtype=float32), array([-0.5272723 ,  0.10032006, 11.955677  ], dtype=float32), array([-0.552325  ,  0.10177733, 18.955976  ], dtype=float32), array([-0.5743871,  0.1022345, 25.956293 ], dtype=float32), array([-0.59345406,  0.10269556, 32.956135  ], dtype=float32))}\n"
     ]
    }
   ],
   "source": [
    "# On teste l'environnement avec des actions random\n",
    "# Normalement si t'es sur WSL le render_mode = 'human' va te causer pb vu qu'il y a pas d'interface graphique. \n",
    "# Faut faire un X11 forwarding, perso j'ai DL XLaunch pour faire ça.\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode='human', track='abyss',max_paths=5)\n",
    "ix = 0\n",
    "done = False\n",
    "state, *_ = env.reset() \n",
    "\n",
    "while not done:\n",
    "    ix += 1\n",
    "    action = env.action_space.sample()\n",
    "    print(env.action_space)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    print(env.observation_space,state)\n",
    "    done = truncated or terminated\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "# Je vais te monter le pb dont je te parlais tout à l'heure lié à pystk2_gymnasium.\n",
    "# En gros, le .reset() est buggué, exemple : \n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False,name=\"Hatem\"),render_mode=None, track='abyss',max_paths=5)\n",
    "done = False\n",
    "state, *_ = env.reset() # Tout roule\n",
    "action = env.action_space.sample()\n",
    "env.step(action)\n",
    "env.reset() # Là normalement t'as une erreur.\n",
    "env.close()\n",
    "\n",
    "# En soi normalement on s'en balek, mais le truc c'est que ce problème va faire crash le PPO de stable baseline plus bas, donc faut le fix.\n",
    "# Pour faire ça, faut aller dans envs.py, et modifier reset. Voilà la modif:\n",
    "\n",
    "# def reset(\n",
    "#         self,\n",
    "#         *,\n",
    "#         seed: Optional[int] = None,\n",
    "#         options: Optional[Dict[str, Any]] = None,\n",
    "#     ) -> Tuple[pystk2.WorldState, Dict[str, Any]]:\n",
    "        \n",
    "#         random = np.random.RandomState(seed)\n",
    "\n",
    "#         # Ajout d'un nettoyage manuel pour self.race\n",
    "#         if self.race:\n",
    "#             print(\"Nettoyage de la course en cours avant le reset.\")\n",
    "#             self.race = None  # Libère la course manuellement pour éviter les conflits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de garder uniquement les observations présentes dans keys_to_keep\n",
    "    \"\"\"\n",
    "    def __init__(self, env, keys_to_keep):\n",
    "        super().__init__(env)\n",
    "        self.keys_to_keep = keys_to_keep\n",
    "        \n",
    "        # Filtrage des dimensions et transformation en espace Box\n",
    "        low = []\n",
    "        high = []\n",
    "        for key in keys_to_keep:\n",
    "            space = self.env.observation_space[key]\n",
    "            low.extend(space.low.flatten())\n",
    "            high.extend(space.high.flatten())\n",
    "        \n",
    "        # Définir le nouvel espace d'observation comme un Box 1D\n",
    "        self.observation_space = spaces.Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Filtrage des clés et conversion en vecteur 1D\n",
    "        filtered_values = [observation[key].flatten() for key in self.keys_to_keep]\n",
    "        return np.concatenate(filtered_values)\n",
    "\n",
    "# Clés à garder dans l'observation (modifiez selon vos préférences)\n",
    "keys_to_keep = [\n",
    "    'velocity',            # Vitesse actuelle du kart\n",
    "    'center_path_distance',# Distance du chemin\n",
    "    'max_steer_angle',     # Angle maximum de direction\n",
    "    'front',\n",
    "    'center_path',\n",
    "    'paths_start', # Empiriquement, ces 5 là changent radicalement l'entraînement : On passe d'un bot qui tombe partout à un truc vraiment correct grâce\n",
    "    'paths_width', # à eux. \n",
    "    'paths_end'\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToBoxActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper permettant de passer d'un dictionnaire d'actions à des actions sous forme matricielle pour PPO après\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DictToBoxActionWrapper, self).__init__(env)\n",
    "        # Définir l'espace d'action en tant que Box 1D qui combine l'accélération et la direction\n",
    "        self.action_space = Box(low=np.array([0.0, -1.0]), high=np.array([1.0, 1.0]), dtype=np.float32)\n",
    "\n",
    "    def action(self, action):\n",
    "        # Diviser l'action en 'acceleration' et 'steer' à partir du vecteur 1D\n",
    "        return {'acceleration': np.array([action[0]], dtype=np.float32), 'steer': np.array([action[1]], dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "Arrêt de la course précédente avant le warmup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatem/deepdac/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:453: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrêt de la course précédente avant le warmup.\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006274202 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0143     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.0903      |\n",
      "-----------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -137         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 57           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067319777 |\n",
      "|    clip_fraction        | 0.0658       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.21         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00881     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -138         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 56           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052015525 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0113       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00723     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 0.0903       |\n",
      "------------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008994948 |\n",
      "|    clip_fraction        | 0.0581      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 0.053       |\n",
      "-----------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006901379 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0085     |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009633677 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007255395 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0806      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00836    |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | -127         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 56           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 324          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057640644 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00971     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00802     |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 0.135        |\n",
      "------------------------------------------\n",
      "Arrêt de la course précédente avant le warmup.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006335214 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00967    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    std                  | 0.948       |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fb9d782c700>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=None,track='abyss')\n",
    "# Entraînement uniquement sur la map 'abyss' pour le moment\n",
    "env = OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env)))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "# En gros, ConstantSizedObservations permet de passer à un environnement \"réduit\", voir la doc en vrai c'est bien expliqué\n",
    "# PolarObservations ajoute des observations utiles (passage en coordonnées polaire pour avoir max_steer_angle notamment)\n",
    "\n",
    "check_env(env,warn=True) # Permet de check si tout est en ordre\n",
    "\n",
    "# Configuration et lancement de PPO\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # Utilise un réseau de neurones entièrement connecté (MLP) comme politique\n",
    "    env,            \n",
    "    verbose=1,      # Active les messages de log pour suivre l'entraînement\n",
    "    n_steps=2048,   # Nombre d'étapes dans chaque mise à jour de PPO\n",
    "    batch_size=64,  # Taille des batchs\n",
    "    gae_lambda=0.95, # Paramètre pour l'estimation de l'avantage généralisé\n",
    "    gamma=0.99,     \n",
    "    ent_coef=0.01,  \n",
    "    learning_rate=2.5e-4,  \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=20000)\n",
    "\n",
    "# model.save(\"../models/ppo_supertuxkart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"../models/ppo_supertuxkart\")\n",
    "\n",
    "env = gym.make(\"supertuxkart/full-v0\", agent=AgentSpec(use_ai=False, name=\"Hatem\"),render_mode=\"human\")\n",
    "env = OnlyContinuousActionsWrapper(PolarObservations(ConstantSizedObservations(env)))\n",
    "env = FilteredObservationWrapper(env, keys_to_keep)\n",
    "env = DictToBoxActionWrapper(env)\n",
    "\n",
    "state,_ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "deepdac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
